# -------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for
# license information.
# --------------------------------------------------------------------------

import torch
from onnxconverter_common.data_types import TensorType


class RawModelContainerNode(object):
    """
    This node is the carrier of the model we want to convert.
    It provides an abstract layer so that our parsing
    framework can work with models generated by different tools.
    """

    def __init__(self, raw_model):
        """
        :param raw_model: the model to convert
        """
        self._raw_model = raw_model

    @property
    def raw_model(self):
        return self._raw_model

    @property
    def input_names(self):
        """
        This function should return a list of strings. Each string
        corresponds to an input variable name.
        :return: a list of string
        """
        raise NotImplementedError()

    @property
    def output_names(self):
        """
        This function should return a list of strings. Each string
        corresponds to an output variable name.
        :return: a list of string
        """
        raise NotImplementedError()


class SklearnModelContainerNode(RawModelContainerNode):
    """
    Main container for a *scikit-learn* model.
    Every converter adds nodes to an existing container
    which is the converted into a tensor graph by an instance of
    :class:`Topology <hummingbird.common._topology.Topology>`.
    """

    def __init__(self, sklearn_model):
        super(SklearnModelContainerNode, self).__init__(sklearn_model)
        # Scikit-learn models have no input and output specified,
        # so we create them and store them in this container.
        self._inputs = []
        self._outputs = []

    @property
    def input_names(self):
        return [variable.raw_name for variable in self._inputs]

    @property
    def output_names(self):
        # Raw names are non unique. Hence using backend names.
        return [variable.backend_name for variable in self._outputs]

    def add_input(self, variable):
        # The order of adding variables matters. The final model's
        # input names are sequentially added as this list
        if variable not in self._inputs:
            self._inputs.append(variable)

    def add_output(self, variable):
        # The order of adding variables matters. The final model's
        # output names are sequentially added as this list
        if variable not in self._outputs:
            self._outputs.append(variable)


class PyTorchBackendModel(torch.nn.Module):
    """
    Container for a model compiled for the PyTorch Backend.
    """

    def __init__(self, input_names, output_names, operator_map, topology, extra_config):
        super(PyTorchBackendModel, self).__init__()
        self.input_names = input_names
        self.output_names = output_names
        self.operator_map = torch.nn.ModuleDict(operator_map)
        self.topology = topology
        self.extra_config = extra_config

    def forward(self, *pytorch_inputs):
        with torch.no_grad():
            pytorch_inputs = [*pytorch_inputs]
            variable_map = {}

            # Maps data inputs to the expected variables.
            for i, input_name in enumerate(self.input_names):
                variable_map[input_name] = pytorch_inputs[i]

            # Evaluate all the operators in the topology by properly wiring inputs \ outputs
            for operator in self.topology.topological_operator_iterator():
                pytorch_op = self.operator_map[operator.full_name]
                pytorch_outputs = pytorch_op(*(variable_map[input.backend_name] for input in operator.inputs))

                if len(operator.outputs) == 1:
                    variable_map[operator.outputs[0].backend_name] = pytorch_outputs
                else:
                    for i, output in enumerate(operator.outputs):
                        variable_map[output.backend_name] = pytorch_outputs[i]

            # Prepare and return the output.
            if len(self.output_names) == 1:
                return variable_map[self.output_names[0]]
            else:
                return list(variable_map[output_name] for output_name in self.output_names)
